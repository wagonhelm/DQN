{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    # Class Attributes\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Network Parameters\n",
    "    \n",
    "    \n",
    "    @classmethod  \n",
    "    def create(cls, env, neurons, activ):\n",
    "        # Create Environment\n",
    "        cls.env = gym.make(env)\n",
    "        cls.n_inputs = cls.env.observation_space.shape[0]\n",
    "        cls.n_actions = cls.env.action_space.n\n",
    "        cls.n_layers = [neurons]\n",
    "        cls.activ = activ\n",
    "        \n",
    "    @classmethod\n",
    "    def initialize(cls):\n",
    "        DQN.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Define Variable Swap\n",
    "        cls.tar_w = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'tar')\n",
    "        cls.pol_w = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'pol')\n",
    "        cls.assign_op = [tf.assign(a,b) for a, b in zip(cls.tar_w,cls.pol_w)]\n",
    "              \n",
    "    def __init__(self, name, trainable):\n",
    "        # Instance Attributes\n",
    "        self.name = name\n",
    "        self.trainable = trainable\n",
    "        self.X = tf.placeholder(tf.float32, (None,DQN.n_inputs), name='state')\n",
    "        if trainable:\n",
    "            self.A = tf.placeholder(tf.int32, name='action')\n",
    "            # One Hot Encoding Op\n",
    "            self.onehot = tf.one_hot(self.A, DQN.n_actions)\n",
    "        else:\n",
    "            self.R = tf.placeholder(tf.float32, name='reward')\n",
    "            self.D = tf.placeholder(tf.float32, name='done')\n",
    "        \n",
    "    def buildNet(self):\n",
    "        # Network\n",
    "        self.layer1 = tf.layers.dense(\n",
    "                      inputs = self.X, \n",
    "                      units = DQN.n_layers[0], \n",
    "                      activation = DQN.activ,\n",
    "                      trainable = self.trainable,\n",
    "                      name = self.name+'_layer1')\n",
    "        \n",
    "        self.logits = tf.layers.dense(\n",
    "                      inputs = self.layer1,\n",
    "                      units = DQN.n_actions,\n",
    "                      trainable = self.trainable,\n",
    "                      name = self.name+'_logits')\n",
    "        \n",
    "        with tf.variable_scope(self.name+'_q_values'):\n",
    "            if self.trainable:\n",
    "                self.action = tf.argmax(self.logits, axis=1)\n",
    "                self.value = tf.reduce_sum(self.onehot * self.logits)\n",
    "            else:\n",
    "                self.argmax = tf.reduce_max(self.logits,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPS:\n",
    "    \n",
    "    def __init__(self, gamma, alpha):\n",
    "        \n",
    "        # Loss Ops\n",
    "        self.get_yj = tar.R + gamma * tar.argmax * tar.D\n",
    "        self.loss = tf.losses.huber_loss(self.get_yj,pol.value)\n",
    "        \n",
    "        # Training Ops\n",
    "        self.optimizer = tf.train.AdamOptimizer(alpha)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(\"/tmp/DQN\")\n",
    "        tf.summary.histogram('target_weights', tf.global_variables()[0])\n",
    "        tf.summary.histogram('target_logits', tf.global_variables()[2])\n",
    "        tf.summary.histogram('policy_weights', tf.global_variables()[4])\n",
    "        tf.summary.histogram('policy_logits', tf.global_variables()[6])\n",
    "        tf.summary.scalar('Total_Loss', self.loss)\n",
    "        self.write_op = tf.summary.merge_all()\n",
    "        self.writer.add_graph(DQN.sess.graph)\n",
    "        \n",
    "        DQN.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \n",
    "    def __init__(self, target_len, memory, batch_max, \n",
    "                 batch_min, epsilon, e_decay):\n",
    "        # Training Hyperparemeters\n",
    "        self.target_len = target_len\n",
    "        self.memory = memory\n",
    "        self.batch_max = batch_max\n",
    "        self.batch_min = batch_min\n",
    "        self.batch_size = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.e_decay = e_decay\n",
    "        \n",
    "        # Bookkeeping\n",
    "        self.batch = []\n",
    "        self.step = 1\n",
    "        self.n = 1\n",
    "        self.Rhist = []\n",
    "        self.Lhist = []\n",
    "        \n",
    "    def egreedy(self,state):\n",
    "        if rand.random() < self.epsilon:\n",
    "            action = DQN.env.action_space.sample()\n",
    "            self.epsilon -= self.e_decay\n",
    "        else:\n",
    "            action = DQN.sess.run(pol.action, feed_dict={pol.X: [state]})[0]\n",
    "        return action\n",
    "    \n",
    "    def backPass(self,batch,done):\n",
    "        batch = batch[np.random.choice(batch.shape[0], \n",
    "                                  self.batch_size, \n",
    "                                  replace=False)]\n",
    "        \n",
    "        state = np.asarray(list(batch[:,0]))\n",
    "        state2 =  np.asarray(list(batch[:,1]))                         \n",
    "        action = batch[:,2]\n",
    "        reward = batch[:,3]\n",
    "        done = [int(not d) for d in batch[:,4]] \n",
    "                                  \n",
    "        feed = {pol.X: state,\n",
    "                tar.X: state2,\n",
    "                pol.A: action,\n",
    "                tar.R: reward,\n",
    "                tar.D: done}\n",
    "                                  \n",
    "        DQN.sess.run(ops.train_op,feed_dict=feed)\n",
    "        \n",
    "        #if done and self.n % 5 == 0:\n",
    "            \n",
    "            #summary = DQN.sess.run(ops.write_op, feed_dict=feed)\n",
    "            #ops.writer.add_summary(summary, self.n)\n",
    "            #ops.writer.flush()\n",
    "        \n",
    "        losses = DQN.sess.run(ops.loss, feed_dict=feed)\n",
    "        return losses\n",
    "                                  \n",
    "    def decideTrain(self):\n",
    "        return len(self.batch) > self.batch_min\n",
    "                                  \n",
    "    def start(self, episodes, render):\n",
    "        for episode in range(episodes):\n",
    "            # Reset Environment\n",
    "            state = DQN.env.reset()                     \n",
    "            # Bookkeeping\n",
    "            rewardHist = []\n",
    "            lossHist = []\n",
    "            frames = 0\n",
    "            \n",
    "            while True:\n",
    "                if render:\n",
    "                    DQN.env.render()\n",
    "                                  \n",
    "                # Get action from policy net and perform\n",
    "                action = self.egreedy(state)\n",
    "                state2, reward, done, _ = DQN.env.step(action)\n",
    "                \n",
    "                # Overide Reward\n",
    "                if len(rewardHist) != 200 and done:\n",
    "                    reward = -1\n",
    "                \n",
    "                # More Bookkeeping\n",
    "                self.batch.append([state, state2, action, reward, done])\n",
    "                self.batch_size = min(len(self.batch), self.batch_max)\n",
    "                rewardHist.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    if self.decideTrain():\n",
    "                        lossHist.append(self.backPass(np.asarray(self.batch), done))\n",
    "                    break\n",
    "                    \n",
    "                if self.decideTrain():\n",
    "                    lossHist.append(self.backPass(np.asarray(self.batch), done))\n",
    "                \n",
    "                # Trim Memory\n",
    "                if len(self.batch) >= self.memory:\n",
    "                    del self.batch[0]\n",
    "                \n",
    "                # Copy policy weights to target\n",
    "                if self.decideTrain() and self.step % self.target_len == 0:\n",
    "                    DQN.sess.run(DQN.assign_op)\n",
    "                 \n",
    "                state = state2\n",
    "                self.step+=1\n",
    "                frames += 1\n",
    "                                    \n",
    "            self.n += 1\n",
    "            if self.decideTrain():\n",
    "                self.Rhist.append(np.sum(rewardHist))\n",
    "                self.Lhist.append(np.mean(lossHist))\n",
    "            if self.n % 50 == 0:\n",
    "                print('Average Reward: {} Average Loss: {}  Epsilon {}'.format(np.mean(self.Rhist), format(np.mean(self.Lhist)), self.epsilon))\n",
    "                if self.n > 100:\n",
    "                    print('Last 100 reward: {}'.format(np.mean(self.Rhist[self.n-100:self.n])))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.create(env='CartPole-v0',\n",
    "           neurons=128,\n",
    "           activ=tf.nn.relu)\n",
    "\n",
    "pol = DQN(name = 'pol', \n",
    "          trainable = True)\n",
    "\n",
    "tar = DQN(name = 'tar', \n",
    "          trainable = False)\n",
    "\n",
    "tar.buildNet()\n",
    "pol.buildNet()\n",
    "DQN.initialize()\n",
    "\n",
    "ops = OPS(gamma=0.999, alpha=1e-3)\n",
    "\n",
    "train = Train(target_len = 2000,\n",
    "              memory = 2000,\n",
    "              batch_max = 1,\n",
    "              batch_min = 1,\n",
    "              epsilon = 0.8,\n",
    "              e_decay =  0.00002)\n",
    "\n",
    "train.start(episodes=10000, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
